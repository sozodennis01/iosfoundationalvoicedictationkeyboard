{"schemaVersion":{"major":0,"minor":3,"patch":0},"metadata":{"images":[{"identifier":"improving-safety-of-model-output-PageImage-card.png","type":"card"}],"roleHeading":"Article","role":"article","modules":[{"name":"Foundation Models"}],"title":"Improving the safety of generative model output"},"kind":"article","seeAlsoSections":[{"generated":true,"anchor":"Essentials","title":"Essentials","identifiers":["doc://com.apple.foundationmodels/documentation/FoundationModels/generating-content-and-performing-tasks-with-foundation-models","doc://com.apple.foundationmodels/documentation/FoundationModels/supporting-languages-and-locales-with-foundation-models","doc://com.apple.foundationmodels/documentation/FoundationModels/adding-intelligent-app-features-with-generative-models","doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel","doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel/UseCase"]}],"sections":[],"primaryContentSections":[{"kind":"content","content":[{"text":"Overview","level":2,"anchor":"overview","type":"heading"},{"inlineContent":[{"text":"Generative AI models have powerful creativity, but with this creativity comes the","type":"text"},{"type":"text","text":" "},{"text":"risk of unintended or unexpected results. For any generative AI feature, safety","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"needs to be an essential part of your design."}],"type":"paragraph"},{"type":"paragraph","inlineContent":[{"text":"The Foundation Models framework has two base layers of safety, where the framework","type":"text"},{"type":"text","text":" "},{"type":"text","text":"uses:"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"An on-device language model that has training to handle sensitive topics"},{"type":"text","text":" "},{"type":"text","text":"with care."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"emphasis","inlineContent":[{"text":"Guardrails","type":"text"}]},{"text":" that aim to block harmful or sensitive content, such as","type":"text"},{"type":"text","text":" "},{"text":"self-harm, violence, and adult materials, from both model input and output.","type":"text"}]}]}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Because safety risks are often contextual, some harms might bypass both built-in"},{"type":"text","text":" "},{"type":"text","text":"framework safety layers. It’s vital to design additional safety layers specific"},{"type":"text","text":" "},{"type":"text","text":"to your app. When developing your feature, decide what’s acceptable or might"},{"type":"text","text":" "},{"type":"text","text":"be harmful in your generative AI feature, based on your app’s use case, cultural"},{"type":"text","text":" "},{"type":"text","text":"context, and audience."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"For more information on designing generative AI experiences responsibly, see"},{"type":"text","text":" "},{"type":"text","text":"Human Interface Guidelines > Foundations > "},{"identifier":"https://developer.apple.com/design/human-interface-guidelines/generative-ai","type":"reference","isActive":true},{"type":"text","text":"."}]},{"level":2,"text":"Handle guardrail errors","type":"heading","anchor":"Handle-guardrail-errors"},{"inlineContent":[{"type":"text","text":"When you send a prompt to the model, "},{"type":"reference","identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel/Guardrails","isActive":true},{"type":"text","text":" check"},{"type":"text","text":" "},{"text":"the input prompt and the model’s output. If either fails the guardrail’s safety check,","type":"text"},{"type":"text","text":" "},{"type":"text","text":"the model session throws a "},{"type":"reference","identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/GenerationError/guardrailViolation(_:)","isActive":true},{"type":"text","text":" "},{"text":"error:","type":"text"}],"type":"paragraph"},{"type":"codeListing","syntax":"swift","code":["do {","    let session = LanguageModelSession()","    let topic = // A potentially harmful topic.","    let prompt = \"Write a respectful and funny story about \\(topic).\"","    let response = try await session.respond(to: prompt)","} catch LanguageModelSession.GenerationError.guardrailViolation {","    // Handle the safety error.","}"]},{"inlineContent":[{"type":"text","text":"If you encounter a guardrail violation error for any built-in prompt in your app,"},{"type":"text","text":" "},{"type":"text","text":"experiment with re-phrasing the prompt to determine which phrases are activating"},{"text":" ","type":"text"},{"text":"the guardrails, and avoid those phrases. If the error is thrown in response to","type":"text"},{"type":"text","text":" "},{"type":"text","text":"a prompt created by someone using your app, give people a clear message that"},{"type":"text","text":" "},{"text":"explains the issue. For example, you might say “Sorry, this feature isn’t designed","type":"text"},{"type":"text","text":" "},{"text":"to handle that kind of input” and offer people the opportunity to try a different","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"prompt."}],"type":"paragraph"},{"level":2,"anchor":"Handle-model-refusals","text":"Handle model refusals","type":"heading"},{"inlineContent":[{"text":"The on-device language model may not be suitable for handling all requests and","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"may refuse requests for a topic. When you generate a string response, and the"},{"text":" ","type":"text"},{"type":"text","text":"model refuses a request, it generates a message that begins with a refusal like"},{"type":"text","text":" "},{"type":"text","text":"“Sorry, I can’t help with”."}],"type":"paragraph"},{"type":"paragraph","inlineContent":[{"text":"Design your app experience with refusal messages in mind and present the message","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"to the person using your app. You might not be able to programmatically determine"},{"type":"text","text":" "},{"text":"whether a string response is a normal response or a refusal, so design the","type":"text"},{"type":"text","text":" "},{"text":"experience to anticipate both. If it’s critical to determine whether the response is a","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"refusal message, initialize a new "},{"identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession","type":"reference","isActive":true},{"text":" and prompt the model","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"to classify whether the string is a refusal."}]},{"inlineContent":[{"text":"When you use guided generation to generate Swift structures or types, there’s no","type":"text"},{"type":"text","text":" "},{"type":"text","text":"placeholder for a refusal message. Instead, the model throws a "},{"type":"reference","identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/GenerationError/refusal(_:_:)","isActive":true},{"text":" ","type":"text"},{"text":"error. When you catch the error, you can ask the model to generate a string","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"refusal message:"}],"type":"paragraph"},{"syntax":"swift","code":["do {","    let session = LanguageModelSession()","    let topic = \"\"  // A sensitive topic.","    let response = try session.respond(","        to: \"List five key points about: \\(topic)\",","        generating: [String].self","    )","} catch LanguageModelSession.GenerationError.refusal(let refusal, _) {","    // Generate an explanation for the refusal.","    if let message = try? await refusal.explanation {","        // Display the refusal message.","    }","}"],"type":"codeListing"},{"inlineContent":[{"type":"text","text":"Display the explanation in your app to tell people why a request failed, and"},{"text":" ","type":"text"},{"text":"offer people the opportunity to try a different prompt. Retrieving an explanation","type":"text"},{"type":"text","text":" "},{"text":"message is asynchronous and takes time for the model to generate.","type":"text"}],"type":"paragraph"},{"type":"paragraph","inlineContent":[{"type":"text","text":"If you encounter a refusal message, or refusal error, for any built-in prompts in"},{"type":"text","text":" "},{"text":"your app, experiment with re-phrasing your prompt to avoid any sensitive topics","type":"text"},{"text":" ","type":"text"},{"text":"that might cause the refusal.","type":"text"}]},{"inlineContent":[{"type":"text","text":"For more information about guided generation, see"},{"text":" ","type":"text"},{"type":"reference","identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation","isActive":true},{"text":".","type":"text"}],"type":"paragraph"},{"text":"Build boundaries on input and output","anchor":"Build-boundaries-on-input-and-output","level":2,"type":"heading"},{"inlineContent":[{"type":"text","text":"Safety risks increase when a prompt includes direct input from a person using"},{"type":"text","text":" "},{"text":"your app, or from an unverified external source, like a webpage. An untrusted","type":"text"},{"type":"text","text":" "},{"type":"text","text":"source makes it difficult to anticipate what the input contains. Whether"},{"text":" ","type":"text"},{"type":"text","text":"accidentally or on purpose, someone could input sensitive content that causes"},{"type":"text","text":" "},{"type":"text","text":"the model to respond poorly."}],"type":"paragraph"},{"name":"Tip","type":"aside","content":[{"type":"paragraph","inlineContent":[{"text":"The more you can define the intended usage and outcomes for your feature,","type":"text"},{"type":"text","text":" "},{"text":"the more you can ensure generation works great for your app’s specific use cases.","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"Add boundaries to limit out-of-scope usage and minimize low generation quality"},{"type":"text","text":" "},{"text":"from out-of-scope uses.","type":"text"}]}],"style":"tip"},{"inlineContent":[{"type":"text","text":"Whenever possible, avoid open input in prompts and place boundaries for controlling"},{"text":" ","type":"text"},{"type":"text","text":"what the input can be. This approach helps when you want generative content to"},{"type":"text","text":" "},{"type":"text","text":"stay within the bounds of a particular topic or task. For the highest level of"},{"type":"text","text":" "},{"text":"safety on input, give people a fixed set of prompts to choose from. This gives","type":"text"},{"text":" ","type":"text"},{"text":"you the highest certainty that sensitive content won’t make its way into your app:","type":"text"}],"type":"paragraph"},{"syntax":"swift","type":"codeListing","code":["enum TopicOptions {","    case family","    case nature","    case work ","}","let topicChoice = TopicOptions.nature","let prompt = \"\"\"","    Generate a wholesome and empathetic journal prompt that helps \\","    this person reflect on \\(topicChoice)","    \"\"\""]},{"inlineContent":[{"text":"If your app allows people to freely input a prompt, placing boundaries on","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"the output can also offer stronger safety guarantees. Using guided generation,"},{"type":"text","text":" "},{"type":"text","text":"create an enumeration to restrict the model’s output to a set of predefined"},{"text":" ","type":"text"},{"text":"options designed to be safe no matter what:","type":"text"}],"type":"paragraph"},{"code":["@Generable","enum Breakfast {","    case waffles","    case pancakes","    case bagels","    case eggs ","}","let session = LanguageModelSession()","let userInput = \"I want something sweet.\"","let prompt = \"Pick the ideal breakfast for request: \\(userInput)\"","let response = try await session.respond(to: prompt, generating: Breakfast.self)"],"syntax":"swift","type":"codeListing"},{"level":2,"anchor":"Instruct-the-model-for-added-safety","type":"heading","text":"Instruct the model for added safety"},{"inlineContent":[{"text":"Consider adding detailed session ","type":"text"},{"isActive":true,"identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/Instructions","type":"reference"},{"text":" that tell the model how to handle","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"sensitive content. The language model prioritizes following its instructions"},{"text":" ","type":"text"},{"type":"text","text":"over any prompt, so instructions are an effective tool for improving safety and"},{"text":" ","type":"text"},{"text":"overall generation quality. Use uppercase words to emphasize the importance of","type":"text"},{"type":"text","text":" "},{"type":"text","text":"certain phrases for the model:"}],"type":"paragraph"},{"type":"codeListing","syntax":"swift","code":["do {","    let instructions = \"\"\"","        ALWAYS respond in a respectful way. \\","        If someone asks you to generate content that might be sensitive, \\","        you MUST decline with 'Sorry, I can't do that.'","        \"\"\"","    let session = LanguageModelSession(instructions: instructions)","    let prompt = // Open input from a person using the app.","    let response = try await session.respond(to: prompt)","} catch LanguageModelSession.GenerationError.guardrailViolation {","    // Handle the safety error.","}"]},{"style":"note","content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"A session obeys instructions over a prompt, so don’t include input from"},{"type":"text","text":" "},{"type":"text","text":"people or any unverified input in the instructions. Using unverified input in"},{"text":" ","type":"text"},{"type":"text","text":"instructions makes your app vulnerable to prompt injection attacks, so write"},{"type":"text","text":" "},{"type":"text","text":"instructions with content you trust."}]}],"type":"aside","name":"Note"},{"type":"paragraph","inlineContent":[{"type":"text","text":"If you want to include open-input from people, instructions for safety are recommended."},{"type":"text","text":" "},{"text":"For an additional layer of safety, use a format string in normal prompts that","type":"text"},{"type":"text","text":" "},{"type":"text","text":"wraps people’s input in your own content that specifies how the model should respond:"}]},{"syntax":"swift","type":"codeListing","code":["let userInput = // The input a person enters in the app.","let prompt = \"\"\"","    Generate a wholesome and empathetic journal prompt that helps \\","    this person reflect on their day. They said: \\(userInput)","    \"\"\""]},{"anchor":"Add-a-deny-list-of-blocked-terms","type":"heading","level":2,"text":"Add a deny list of blocked terms"},{"inlineContent":[{"type":"text","text":"If you allow prompt input from people or outside sources, consider adding your"},{"text":" ","type":"text"},{"text":"own deny list of terms. A deny list is anything you don’t want people to be","type":"text"},{"type":"text","text":" "},{"type":"text","text":"able to input to your app, including unsafe terms, names of people or products,"},{"text":" ","type":"text"},{"text":"or anything that’s not relevant to the feature you provide. Implement a deny","type":"text"},{"type":"text","text":" "},{"type":"text","text":"list similarly to guardrails by creating a function that checks the input and"},{"text":" ","type":"text"},{"type":"text","text":"the model output:"}],"type":"paragraph"},{"type":"codeListing","syntax":"swift","code":["let session = LanguageModelSession()","let userInput = // The input a person enters in the app.","let prompt = \"Generate a wholesome story about: \\(userInput)\"","","// A function you create that evaluates whether the input ","// contains anything in your deny list.","if verifyText(prompt) { ","    let response = try await session.respond(to: prompt)","    ","    // Compare the output to evaluate whether it contains anything in your deny list.","    if verifyText(response.content) { ","        return response ","    } else {","        // Handle the unsafe output.","    }","} else {","    // Handle the unsafe input.","}"]},{"inlineContent":[{"text":"A deny list can be a simple list of strings in your code that you distribute with","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"your app. Alternatively, you can host a deny list on a server so your app can"},{"type":"text","text":" "},{"text":"download the latest deny list when it’s connected to the network. Hosting your","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"deny list allows you to update your list when you need to and avoids requiring"},{"text":" ","type":"text"},{"type":"text","text":"a full app update if a safety issue arise."}],"type":"paragraph"},{"anchor":"Use-permissive-guardrail-mode-for-sensitive-content","type":"heading","text":"Use permissive guardrail mode for sensitive content","level":2},{"inlineContent":[{"text":"The default ","type":"text"},{"identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel","isActive":true,"type":"reference"},{"text":" guardrails may throw a","type":"text"},{"text":" ","type":"text"},{"type":"reference","identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/GenerationError/guardrailViolation(_:)","isActive":true},{"type":"text","text":" error for"},{"text":" ","type":"text"},{"type":"text","text":"sensitive source material. For example, it may be appropriate for your app to"},{"type":"text","text":" "},{"text":"work with certain inputs from people and unverified sources that might contain","type":"text"},{"type":"text","text":" "},{"text":"sensitive content:","type":"text"}],"type":"paragraph"},{"items":[{"content":[{"type":"paragraph","inlineContent":[{"text":"When you want the model to tag the topic of conversations in a chat app when","type":"text"},{"text":" ","type":"text"},{"text":"some messages contain profanity.","type":"text"}]}]},{"content":[{"inlineContent":[{"text":"When you want to use the model to explain notes in your study app that discuss","type":"text"},{"type":"text","text":" "},{"type":"text","text":"sensitive topics."}],"type":"paragraph"}]}],"type":"unorderedList"},{"inlineContent":[{"text":"To allow the model to reason about sensitive source material,","type":"text"},{"type":"text","text":" "},{"text":"use ","type":"text"},{"type":"reference","identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel/Guardrails/permissiveContentTransformations","isActive":true},{"text":" when you","type":"text"},{"type":"text","text":" "},{"type":"text","text":"initialize "},{"identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel","type":"reference","isActive":true},{"text":":","type":"text"}],"type":"paragraph"},{"syntax":"swift","code":["let model = SystemLanguageModel(guardrails: .permissiveContentTransformations)"],"type":"codeListing"},{"type":"paragraph","inlineContent":[{"text":"This mode only works for generating a string value. When you use guided generation,","type":"text"},{"text":" ","type":"text"},{"text":"the framework runs the default guardrails against model input and output as","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"usual, and generates "},{"isActive":true,"type":"reference","identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/GenerationError/guardrailViolation(_:)"},{"type":"text","text":""},{"type":"text","text":" "},{"text":"and ","type":"text"},{"isActive":true,"type":"reference","identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/GenerationError/refusal(_:_:)"},{"type":"text","text":"errors as usual."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Before you use permissive content mode, consider what’s appropriate for your"},{"type":"text","text":" "},{"text":"audience. The session skips the guardrail checks in this mode, so it never throws","type":"text"},{"type":"text","text":" "},{"type":"text","text":"a "},{"identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/GenerationError/guardrailViolation(_:)","isActive":true,"type":"reference"},{"type":"text","text":" error when"},{"type":"text","text":" "},{"type":"text","text":"generating string responses."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"However, even with the "},{"isActive":true,"type":"reference","identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel"},{"type":"text","text":" guardrails off, the on-device"},{"type":"text","text":" "},{"text":"system language model still has a layer of safety. For some content, the model","type":"text"},{"type":"text","text":" "},{"text":"may still produce a refusal message that’s similar to, “Sorry, I can’t help with.”","type":"text"}]},{"text":"Create a risk assessment","type":"heading","level":2,"anchor":"Create-a-risk-assessment"},{"type":"paragraph","inlineContent":[{"text":"Conduct a risk assessment to proactively address what might go wrong. Risk","type":"text"},{"text":" ","type":"text"},{"text":"assessment is an exercise that helps you brainstorm potential safety risks in","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"your app and map each risk to an actionable mitigation. You can write a risk"},{"type":"text","text":" "},{"type":"text","text":"assessment in any format that includes these essential elements:"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"type":"text","text":"List each AI feature in your app."}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"For each feature, list possible safety risks that could occur, even if they seem unlikely."}]}]},{"content":[{"type":"paragraph","inlineContent":[{"text":"For each safety risk, score how serious the harm would be if that thing occurred, from mild to critical.","type":"text"}]}]},{"content":[{"inlineContent":[{"type":"text","text":"For each safety risk, assign a strategy for how you’ll mitigate the risk in your app."}],"type":"paragraph"}]}]},{"inlineContent":[{"type":"text","text":"For example, an app might include one feature with the fixed-choice input pattern"},{"text":" ","type":"text"},{"type":"text","text":"for generation and one feature with the open-input pattern for generation, which"},{"text":" ","type":"text"},{"type":"text","text":"is higher safety risk:"}],"type":"paragraph"},{"header":"row","type":"table","rows":[[[{"type":"paragraph","inlineContent":[{"text":"Feature","type":"text"}]}],[{"type":"paragraph","inlineContent":[{"type":"text","text":"Harm"}]}],[{"type":"paragraph","inlineContent":[{"text":"Severity","type":"text"}]}],[{"type":"paragraph","inlineContent":[{"type":"text","text":"Mitigation"}]}]],[[{"inlineContent":[{"type":"text","text":"Player can input any text to chat with nonplayer characters in the coffee shop."}],"type":"paragraph"}],[{"type":"paragraph","inlineContent":[{"text":"A character might respond in an insensitive or harmful way.","type":"text"}]}],[{"type":"paragraph","inlineContent":[{"text":"Critical","type":"text"}]}],[{"type":"paragraph","inlineContent":[{"type":"text","text":"Instructions and prompting to steer characters responses to be safe; safety testing."}]}]],[[{"type":"paragraph","inlineContent":[{"text":"Image generation of an imaginary dream customer, like a fairy or a frog.","type":"text"}]}],[{"type":"paragraph","inlineContent":[{"type":"text","text":"Generated image could look weird or scary."}]}],[{"inlineContent":[{"type":"text","text":"Mild"}],"type":"paragraph"}],[{"inlineContent":[{"type":"text","text":"Include in the prompt examples of images to generate that are cute and not scary; safety testing."}],"type":"paragraph"}]],[[{"inlineContent":[{"text":"Player can make a coffee from a fixed menu of options.","type":"text"}],"type":"paragraph"}],[{"inlineContent":[{"text":"None identified.","type":"text"}],"type":"paragraph"}],[{"type":"paragraph","inlineContent":[]}],[{"type":"paragraph","inlineContent":[]}]],[[{"type":"paragraph","inlineContent":[{"text":"Generate a review of the coffee the player made, based on the customer’s order.","type":"text"}]}],[{"type":"paragraph","inlineContent":[{"text":"Review could be insulting.","type":"text"}]}],[{"type":"paragraph","inlineContent":[{"text":"Moderate","type":"text"}]}],[{"inlineContent":[{"text":"Instructions and prompting to encourage posting a polite review; safety testing.","type":"text"}],"type":"paragraph"}]]]},{"inlineContent":[{"type":"text","text":"Besides obvious harms, like a poor-quality model output, think about"},{"type":"text","text":" "},{"text":"how your generative AI feature might affect people, including real-world scenarios","type":"text"},{"type":"text","text":" "},{"text":"where someone might act based on information generated by your app.","type":"text"}],"type":"paragraph"},{"level":2,"text":"Write and maintain safety tests","type":"heading","anchor":"Write-and-maintain-safety-tests"},{"type":"paragraph","inlineContent":[{"type":"text","text":"Although most people will interact with your app in respectful ways, it’s"},{"type":"text","text":" "},{"type":"text","text":"important to anticipate possible failure modes where certain input or contexts"},{"type":"text","text":" "},{"text":"could cause the model to generate something harmful. Especially if your app takes","type":"text"},{"type":"text","text":" "},{"type":"text","text":"input from people, test your experience’s safety on input like:"}]},{"type":"unorderedList","items":[{"content":[{"inlineContent":[{"text":"Input that is nonsensical, snippets of code, or random characters.","type":"text"}],"type":"paragraph"}]},{"content":[{"type":"paragraph","inlineContent":[{"type":"text","text":"Input that includes sensitive content."}]}]},{"content":[{"inlineContent":[{"text":"Input that includes controversial topics.","type":"text"}],"type":"paragraph"}]},{"content":[{"inlineContent":[{"text":"Vague or unclear input that could be misinterpreted.","type":"text"}],"type":"paragraph"}]}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Create a list of potentially harmful prompt inputs that you can run as part of"},{"text":" ","type":"text"},{"type":"text","text":"your app’s tests. Include every prompt in your app — even safe ones — as part of"},{"text":" ","type":"text"},{"type":"text","text":"your app testing. For each prompt test, log the timestamp, full input prompt,"},{"type":"text","text":" "},{"type":"text","text":"the model’s response, and whether it activates any built-in safety or mitigations"},{"type":"text","text":" "},{"type":"text","text":"you’ve included in your app. When starting out, manually read the model’s response"},{"text":" ","type":"text"},{"type":"text","text":"on all tests to ensure it meets your design and safety goals. To scale your tests,"},{"text":" ","type":"text"},{"type":"text","text":"consider using a frontier LLM to auto-grade the safety of each prompt. Building a"},{"type":"text","text":" "},{"type":"text","text":"test pipeline for prompts and safety is a worthwhile investment for tracking"},{"type":"text","text":" "},{"type":"text","text":"changes in how your app responds over time."}]},{"type":"paragraph","inlineContent":[{"type":"text","text":"Someone might purposefully attempt to break your feature or produce bad"},{"text":" ","type":"text"},{"text":"output — especially someone who won’t be harmed by their actions. But, keep","type":"text"},{"type":"text","text":" "},{"text":"in mind that it’s very important to identify cases where someone might","type":"text"},{"type":"text","text":" "},{"type":"emphasis","inlineContent":[{"type":"text","text":"accidentally"}]},{"type":"text","text":" be harmed during normal app use."}]},{"style":"tip","type":"aside","name":"Tip","content":[{"type":"paragraph","inlineContent":[{"text":"Prioritize protecting people using your app with good intentions. Accidental","type":"text"},{"type":"text","text":" "},{"type":"text","text":"safety failures often only occur in specific contexts, which make them hard to"},{"type":"text","text":" "},{"text":"identify during testing. Test for a longer series of interactions, and test for","type":"text"},{"type":"text","text":" "},{"type":"text","text":"inputs that could become sensitive only when combined with other aspects of your app."}]}]},{"type":"paragraph","inlineContent":[{"text":"Don’t engage in any testing that could cause you or others harm. Apple’s built-in","type":"text"},{"type":"text","text":" "},{"text":"responsible AI and safety measures, like safety guardrails, are built by experts","type":"text"},{"text":" ","type":"text"},{"text":"with extensive training and support. These built-in measures aim to block egregious","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"harms, allowing you to focus on the borderline harmful cases that need your judgement."},{"text":" ","type":"text"},{"text":"Before conducting any safety testing, ensure that you’re in a safe location and","type":"text"},{"text":" ","type":"text"},{"text":"that you have the health and well-being support you need.","type":"text"}]},{"type":"heading","text":"Report safety concerns","level":2,"anchor":"Report-safety-concerns"},{"inlineContent":[{"type":"text","text":"Somewhere in your app, it’s important to include a way that people can report"},{"text":" ","type":"text"},{"type":"text","text":"potentially harmful content. Continuously monitor the feedback you receive, and"},{"text":" ","type":"text"},{"type":"text","text":"be responsive to quickly handling any safety issues that arise. If someone reports"},{"type":"text","text":" "},{"type":"text","text":"a safety concern that you believe isn’t being properly handled by Apple’s built-in"},{"type":"text","text":" "},{"type":"text","text":"guardrails, report it to Apple with "},{"isActive":true,"identifier":"https://support.apple.com/guide/feedback-assistant/get-started-fbab81460adb/mac","type":"reference"},{"text":".","type":"text"}],"type":"paragraph"},{"type":"paragraph","inlineContent":[{"type":"text","text":"The Foundation Models framework offers utilities for feedback. Use "},{"isActive":true,"type":"reference","identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelFeedback"},{"text":" ","type":"text"},{"type":"text","text":"to retrieve language model session transcripts from people using your app."},{"text":" ","type":"text"},{"text":"After collecting feedback, you can serialize it into a JSON file and include","type":"text"},{"type":"text","text":" "},{"type":"text","text":"it in the report you send with Feedback Assistant."}]},{"level":2,"type":"heading","text":"Monitor safety for model or guardrail updates","anchor":"Monitor-safety-for-model-or-guardrail-updates"},{"inlineContent":[{"type":"text","text":"Apple releases updates to the system model as part of regular OS updates. If you"},{"type":"text","text":" "},{"type":"text","text":"participate in the developer beta program you can test your app with new model"},{"type":"text","text":" "},{"type":"text","text":"version ahead of people using your app. When the model updates, it’s important"},{"type":"text","text":" "},{"type":"text","text":"to re-run your full prompt tests in addition to your adversarial safety tests"},{"text":" ","type":"text"},{"type":"text","text":"because the model’s response may change. Your risk assessment can help you track"},{"text":" ","type":"text"},{"type":"text","text":"any change to safety risks in your app."}],"type":"paragraph"},{"inlineContent":[{"type":"text","text":"Apple may update the built-in guardrails at any time outside of the regular"},{"type":"text","text":" "},{"type":"text","text":"OS update cycle. This is done to rapidly respond, for example, to reported safety"},{"type":"text","text":" "},{"text":"concerns that require a fast response. Include all of the prompts you use in your","type":"text"},{"text":" ","type":"text"},{"text":"app in your test suite, and run tests regularly to identify when prompts start","type":"text"},{"type":"text","text":" "},{"type":"text","text":"activating the guardrails."}],"type":"paragraph"}]}],"hierarchy":{"paths":[["doc://com.apple.documentation/documentation/technologies","doc://com.apple.foundationmodels/documentation/FoundationModels"]]},"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["/documentation/foundationmodels/improving-the-safety-of-generative-model-output"]}],"identifier":{"interfaceLanguage":"swift","url":"doc://com.apple.foundationmodels/documentation/FoundationModels/improving-the-safety-of-generative-model-output"},"abstract":[{"text":"Create generative experiences that appropriately handle sensitive inputs and respect people.","type":"text"}],"references":{"https://developer.apple.com/design/human-interface-guidelines/generative-ai":{"identifier":"https://developer.apple.com/design/human-interface-guidelines/generative-ai","url":"https://developer.apple.com/design/human-interface-guidelines/generative-ai","title":"Generative AI","type":"link","titleInlineContent":[{"text":"Generative AI","type":"text"}]},"doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/GenerationError/guardrailViolation(_:)":{"url":"/documentation/foundationmodels/languagemodelsession/generationerror/guardrailviolation(_:)","role":"symbol","type":"topic","identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/GenerationError/guardrailViolation(_:)","fragments":[{"text":"case","kind":"keyword"},{"text":" ","kind":"text"},{"kind":"identifier","text":"guardrailViolation"},{"kind":"text","text":"("},{"text":"LanguageModelSession","preciseIdentifier":"s:16FoundationModels20LanguageModelSessionC","kind":"typeIdentifier"},{"text":".","kind":"text"},{"kind":"typeIdentifier","text":"GenerationError","preciseIdentifier":"s:16FoundationModels20LanguageModelSessionC15GenerationErrorO"},{"text":".","kind":"text"},{"kind":"typeIdentifier","text":"Context","preciseIdentifier":"s:16FoundationModels20LanguageModelSessionC15GenerationErrorO7ContextV"},{"kind":"text","text":")"}],"kind":"symbol","title":"LanguageModelSession.GenerationError.guardrailViolation(_:)","abstract":[{"text":"An error that indicates the system’s safety guardrails are triggered by content in a","type":"text"},{"text":" ","type":"text"},{"text":"prompt or the response generated by the model.","type":"text"}]},"doc://com.apple.foundationmodels/documentation/FoundationModels/Instructions":{"type":"topic","role":"symbol","kind":"symbol","navigatorTitle":[{"kind":"identifier","text":"Instructions"}],"abstract":[{"type":"text","text":"Details you provide that define the model’s intended behavior on prompts."}],"identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/Instructions","url":"/documentation/foundationmodels/instructions","fragments":[{"text":"struct","kind":"keyword"},{"kind":"text","text":" "},{"text":"Instructions","kind":"identifier"}],"title":"Instructions"},"doc://com.apple.documentation/documentation/technologies":{"abstract":[{"type":"text","text":""}],"role":"overview","type":"topic","kind":"technologies","identifier":"doc://com.apple.documentation/documentation/technologies","title":"Technologies","url":"/documentation/technologies"},"doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession":{"kind":"symbol","title":"LanguageModelSession","role":"symbol","type":"topic","abstract":[{"text":"An object that represents a session that interacts with a language model.","type":"text"}],"navigatorTitle":[{"text":"LanguageModelSession","kind":"identifier"}],"url":"/documentation/foundationmodels/languagemodelsession","fragments":[{"text":"class","kind":"keyword"},{"text":" ","kind":"text"},{"text":"LanguageModelSession","kind":"identifier"}],"identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession"},"doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel/UseCase":{"role":"symbol","type":"topic","abstract":[{"text":"A type that represents the use case for prompting.","type":"text"}],"fragments":[{"text":"struct","kind":"keyword"},{"kind":"text","text":" "},{"text":"UseCase","kind":"identifier"}],"url":"/documentation/foundationmodels/systemlanguagemodel/usecase","title":"SystemLanguageModel.UseCase","identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel/UseCase","navigatorTitle":[{"text":"UseCase","kind":"identifier"}],"kind":"symbol"},"doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelFeedback":{"abstract":[{"type":"text","text":"Feedback appropriate for logging or attaching to Feedback Assistant."}],"role":"symbol","type":"topic","navigatorTitle":[{"text":"LanguageModelFeedback","kind":"identifier"}],"kind":"symbol","fragments":[{"text":"struct","kind":"keyword"},{"kind":"text","text":" "},{"text":"LanguageModelFeedback","kind":"identifier"}],"identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelFeedback","title":"LanguageModelFeedback","url":"/documentation/foundationmodels/languagemodelfeedback"},"doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel":{"type":"topic","navigatorTitle":[{"text":"SystemLanguageModel","kind":"identifier"}],"abstract":[{"type":"text","text":"An on-device large language model capable of text generation tasks."}],"url":"/documentation/foundationmodels/systemlanguagemodel","identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel","kind":"symbol","role":"symbol","fragments":[{"text":"class","kind":"keyword"},{"text":" ","kind":"text"},{"text":"SystemLanguageModel","kind":"identifier"}],"title":"SystemLanguageModel"},"adding-intelligent-app-features-with-generative-models-PageImage-card.png":{"variants":[{"url":"https://docs-assets.developer.apple.com/published/29399e54b8f238bef3b51af88ed97253/adding-intelligent-app-features-with-generative-models-PageImage-card%402x.png","traits":["2x","light"]},{"traits":["2x","dark"],"url":"https://docs-assets.developer.apple.com/published/5b6f36cef31dcb9443d08d1885d21cab/adding-intelligent-app-features-with-generative-models-PageImage-card~dark%402x.png"}],"alt":"A screenshot of an itinerary planning app.","identifier":"adding-intelligent-app-features-with-generative-models-PageImage-card.png","type":"image"},"improving-safety-of-model-output-PageImage-card.png":{"alt":null,"variants":[{"traits":["2x","light"],"url":"https://docs-assets.developer.apple.com/published/c4b1976413fefae6c1b7f1ab749cbb3b/improving-safety-of-model-output-PageImage-card%402x.png"},{"url":"https://docs-assets.developer.apple.com/published/3ee223794e8037bc7a83f0458b2773db/improving-safety-of-model-output-PageImage-card~dark%402x.png","traits":["2x","dark"]}],"type":"image","identifier":"improving-safety-of-model-output-PageImage-card.png"},"foundation-models-framework-landing-PageImage-card.png":{"alt":"An illustration that represents a foundation model.","type":"image","variants":[{"traits":["2x","light"],"url":"https://docs-assets.developer.apple.com/published/3b31c20778305122ef5216e67f6a6e33/foundation-models-framework-landing-PageImage-card%402x.png"},{"url":"https://docs-assets.developer.apple.com/published/f16169705bca6cccc271e074fe6ce38d/foundation-models-framework-landing-PageImage-card~dark%402x.png","traits":["2x","dark"]}],"identifier":"foundation-models-framework-landing-PageImage-card.png"},"doc://com.apple.foundationmodels/documentation/FoundationModels":{"url":"/documentation/foundationmodels","identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels","images":[{"type":"card","identifier":"foundation-models-framework-landing-PageImage-card.png"}],"title":"Foundation Models","role":"collection","type":"topic","kind":"symbol","abstract":[{"type":"text","text":"Perform tasks with the on-device model that specializes in language understanding, structured output, and tool calling."}]},"doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel/Guardrails":{"fragments":[{"kind":"keyword","text":"struct"},{"text":" ","kind":"text"},{"kind":"identifier","text":"Guardrails"}],"type":"topic","title":"SystemLanguageModel.Guardrails","kind":"symbol","abstract":[{"type":"text","text":"Guardrails flag sensitive content from model input and output."}],"url":"/documentation/foundationmodels/systemlanguagemodel/guardrails","role":"symbol","navigatorTitle":[{"text":"Guardrails","kind":"identifier"}],"identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel/Guardrails"},"doc://com.apple.foundationmodels/documentation/FoundationModels/generating-content-and-performing-tasks-with-foundation-models":{"kind":"article","identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/generating-content-and-performing-tasks-with-foundation-models","role":"article","images":[{"identifier":"foundation-models-PageImage-card.png","type":"card"}],"title":"Generating content and performing tasks with Foundation Models","url":"/documentation/foundationmodels/generating-content-and-performing-tasks-with-foundation-models","abstract":[{"text":"Enhance the experience in your app by prompting an on-device large language model.","type":"text"}],"type":"topic"},"doc://com.apple.foundationmodels/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation":{"kind":"article","abstract":[{"type":"text","text":"Create robust apps by describing output you want programmatically."}],"type":"topic","role":"article","identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/generating-swift-data-structures-with-guided-generation","title":"Generating Swift data structures with guided generation","url":"/documentation/foundationmodels/generating-swift-data-structures-with-guided-generation"},"doc://com.apple.foundationmodels/documentation/FoundationModels/supporting-languages-and-locales-with-foundation-models":{"role":"article","identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/supporting-languages-and-locales-with-foundation-models","title":"Supporting languages and locales with Foundation Models","abstract":[{"text":"Generate content in the language people prefer when they interact with your app.","type":"text"}],"type":"topic","url":"/documentation/foundationmodels/supporting-languages-and-locales-with-foundation-models","kind":"article"},"doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/GenerationError/refusal(_:_:)":{"fragments":[{"text":"case","kind":"keyword"},{"kind":"text","text":" "},{"kind":"identifier","text":"refusal"},{"kind":"text","text":"("},{"text":"LanguageModelSession","kind":"typeIdentifier","preciseIdentifier":"s:16FoundationModels20LanguageModelSessionC"},{"text":".","kind":"text"},{"text":"GenerationError","preciseIdentifier":"s:16FoundationModels20LanguageModelSessionC15GenerationErrorO","kind":"typeIdentifier"},{"kind":"text","text":"."},{"text":"Refusal","kind":"typeIdentifier","preciseIdentifier":"s:16FoundationModels20LanguageModelSessionC15GenerationErrorO7RefusalV"},{"text":", ","kind":"text"},{"text":"LanguageModelSession","kind":"typeIdentifier","preciseIdentifier":"s:16FoundationModels20LanguageModelSessionC"},{"text":".","kind":"text"},{"kind":"typeIdentifier","text":"GenerationError","preciseIdentifier":"s:16FoundationModels20LanguageModelSessionC15GenerationErrorO"},{"text":".","kind":"text"},{"preciseIdentifier":"s:16FoundationModels20LanguageModelSessionC15GenerationErrorO7ContextV","text":"Context","kind":"typeIdentifier"},{"kind":"text","text":")"}],"abstract":[{"text":"An error that happens when the session refuses the request.","type":"text"}],"identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/LanguageModelSession/GenerationError/refusal(_:_:)","url":"/documentation/foundationmodels/languagemodelsession/generationerror/refusal(_:_:)","type":"topic","kind":"symbol","title":"LanguageModelSession.GenerationError.refusal(_:_:)","role":"symbol"},"foundation-models-PageImage-card.png":{"alt":"An abstract illustration with lines coming together to form a single line.","variants":[{"traits":["1x","light"],"url":"https://docs-assets.developer.apple.com/published/096a5d179ca102a325db9f643e9bc31e/foundation-models-PageImage-card.png"},{"url":"https://docs-assets.developer.apple.com/published/bd8f99c52038bd092000c924ab2ded3b/foundation-models-PageImage-card~dark.png","traits":["1x","dark"]}],"type":"image","identifier":"foundation-models-PageImage-card.png"},"doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel/Guardrails/permissiveContentTransformations":{"url":"/documentation/foundationmodels/systemlanguagemodel/guardrails/permissivecontenttransformations","abstract":[{"text":"Guardrails that allow for permissively transforming text input, including","type":"text"},{"type":"text","text":" "},{"text":"potentially unsafe content, to text responses, such as summarizing an article.","type":"text"}],"identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/SystemLanguageModel/Guardrails/permissiveContentTransformations","fragments":[{"text":"static","kind":"keyword"},{"kind":"text","text":" "},{"text":"let","kind":"keyword"},{"text":" ","kind":"text"},{"text":"permissiveContentTransformations","kind":"identifier"},{"text":": ","kind":"text"},{"preciseIdentifier":"s:16FoundationModels19SystemLanguageModelC","text":"SystemLanguageModel","kind":"typeIdentifier"},{"text":".","kind":"text"},{"preciseIdentifier":"s:16FoundationModels19SystemLanguageModelC10GuardrailsV","text":"Guardrails","kind":"typeIdentifier"}],"kind":"symbol","role":"symbol","type":"topic","title":"permissiveContentTransformations"},"doc://com.apple.foundationmodels/documentation/FoundationModels/adding-intelligent-app-features-with-generative-models":{"url":"/documentation/foundationmodels/adding-intelligent-app-features-with-generative-models","identifier":"doc://com.apple.foundationmodels/documentation/FoundationModels/adding-intelligent-app-features-with-generative-models","title":"Adding intelligent app features with generative models","abstract":[{"text":"Build robust apps with guided generation and tool calling by adopting the Foundation Models framework.","type":"text"}],"type":"topic","role":"sampleCode","images":[{"type":"card","identifier":"adding-intelligent-app-features-with-generative-models-PageImage-card.png"}],"kind":"article"},"https://support.apple.com/guide/feedback-assistant/get-started-fbab81460adb/mac":{"url":"https://support.apple.com/guide/feedback-assistant/get-started-fbab81460adb/mac","type":"link","title":"Feedback Assistant","identifier":"https://support.apple.com/guide/feedback-assistant/get-started-fbab81460adb/mac","titleInlineContent":[{"text":"Feedback Assistant","type":"text"}]}},"legalNotices":{"copyright":"Copyright &copy; 2026 Apple Inc. All rights reserved.","termsOfUse":"https://www.apple.com/legal/internet-services/terms/site.html","privacyPolicy":"https://www.apple.com/privacy/privacy-policy"}}